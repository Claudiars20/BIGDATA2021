{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6c5b0c",
   "metadata": {},
   "source": [
    "# **Bag of Words**\n",
    "---\n",
    "> **Asignatura:** Minería de Datos\n",
    ">\n",
    "> **Alumna:** Claudia Luz Rojas Soto\n",
    ">\n",
    "> **Código:** 171805\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d1ea7",
   "metadata": {},
   "source": [
    "## **Algoritmos:**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aafee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f37f63",
   "metadata": {},
   "source": [
    "### 1. BoW\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d21f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitarPuntuacion(texto):\n",
    "    \"\"\"Permite eliminar puntuación de un texto(incluidas letras con acentos)\n",
    "    Args:\n",
    "        texto (string): Texto a preprocesar.\n",
    "    Returns:\n",
    "        str: El mismo string pero sin signos de puntuación y tildes.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', texto).strip().lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a509eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(textos):\n",
    "    \"\"\"Permite hallar el BagOfWords de un conjunto de textos(corpus)\n",
    "    Args:\n",
    "        textos (List): Lista de textos.\n",
    "    Returns:\n",
    "        str: Una lista de palabras de todo el corpus\n",
    "    \"\"\"\n",
    "    bow = textos.flatMap(lambda x: x.split()).filter(lambda y: len(y)>2)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ac33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 3) / 3]\r",
      "\r",
      "[Stage 0:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'hola', 'hola', 'como', 'estas', 'hola', 'buenos', 'dias', 'bien', 'bien', 'como', 'estas', 'hola', 'nombre', 'claudia', 'tuyo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "textos = ['Hola! hola hola como estas?'\n",
    "          ,'Hola buenos dias, bien bien y tu como estas?'\n",
    "          ,'Hola, mi nombre es Claudia, y el tuyo?']\n",
    "\n",
    "textos = sc.parallelize(textos,3)\n",
    "preprocesado = textos.map(lambda x: quitarPuntuacion(x))\n",
    "bow = BagOfWords(preprocesado)\n",
    "print(bow.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba0d2d",
   "metadata": {},
   "source": [
    "**Frecuencia de palabras:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca3e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hola', 5), ('nombre', 1), ('claudia', 1), ('tuyo', 1), ('estas', 2), ('dias', 1), ('bien', 2), ('como', 2), ('buenos', 1)]\n"
     ]
    }
   ],
   "source": [
    "frecuencia = bow.map(lambda x: (x,1)).reduceByKey(lambda x,y: x + y)\n",
    "print(frecuencia.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9a1a3",
   "metadata": {},
   "source": [
    "### 2. TF (Term Frequency)\n",
    "---\n",
    "### $$tf(t,d) = 1 + log(f_{t,d})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9e37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tf(tokens):\n",
    "    \"\"\"Permite hallar el TF(Term Frecuency) de un texto.\n",
    "    Args:\n",
    "        tokens (List): Lista de palabras.\n",
    "    Returns:\n",
    "        str: Una lista de tuplas compuesto por (palabra, TF).\n",
    "    \"\"\"\n",
    "    # Recuperamos los valores unicos de la lista de ilter\n",
    "    unicos = list(dict.fromkeys(tokens))\n",
    "    tf = map(lambda x: (x,1+math.log10(tokens.count(x)/len(tokens))),unicos)\n",
    "    return list(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfef900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hola', 'hola', 'hola', 'como', 'estas'], ['hola', 'buenos', 'dias', 'bien', 'bien', 'y', 'tu', 'como', 'estas'], ['hola', 'mi', 'nombre', 'es', 'claudia', 'y', 'el', 'tuyo']]\n"
     ]
    }
   ],
   "source": [
    "# Usaremos el preprocesado anterior como corpus\n",
    "tokens_x_texto = preprocesado.map(lambda x: x.split())\n",
    "print(tokens_x_texto.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6d6967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hola', 0.7781512503836436), ('como', 0.30102999566398125), ('estas', 0.30102999566398125)]\n"
     ]
    }
   ],
   "source": [
    "TF = tokens_x_texto.map(lambda x: tf(x))\n",
    "print(TF.collect()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f838c",
   "metadata": {},
   "source": [
    "### 3. TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d531f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulo para contar palabras\n",
    "def contarPalabras(palabraRDD):\n",
    "    \"\"\"Crea un par RDD con cada palabra y su contador.\n",
    "\n",
    "    Args:\n",
    "        palabraRDD (RDD de str): RDD de palabras.\n",
    "\n",
    "    Returns:\n",
    "        RDD (str, int): Un RDD que consiste en (word, count) tuplas.\n",
    "    \"\"\"\n",
    "    return (palabraRDD\n",
    "            .map(lambda x: (x,1))\n",
    "            .reduceByKey(lambda x,y: x + y)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0e462",
   "metadata": {},
   "source": [
    "### **IDF** (Inverse Document Frequency): \n",
    "    \n",
    "###    $$idf(t,D) = log(1 + \\frac{N}{n_t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417f05ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola hola hola como estas', 'hola buenos dias bien bien y tu como estas', 'hola mi nombre es claudia y el tuyo']\n"
     ]
    }
   ],
   "source": [
    "# Usaremos el preprocesado como corpus\n",
    "print(preprocesado.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88984951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(corpus):\n",
    "    \"\"\"Permite hallar el IDF(Inverse Document Frecuency) de las palabras de un corpus.\n",
    "    Args:\n",
    "        corpus (RDD list): Lista de textos.\n",
    "    Returns:\n",
    "        (RDD list): Una lista de tuplas compuesto por (palabra, IDF).\n",
    "    \"\"\"\n",
    "    # Tamaño del corpus\n",
    "    tam = len(corpus.collect())\n",
    "    # Recogemos las palabras unicas por documento dentro de todo el corpus\n",
    "    unicas = corpus.flatMap(lambda x: set(x.split()))\n",
    "    # Con esto nos aseguramos de contar la aparicion de una plabra dentro de todo el corpus\n",
    "    # Solo contando el numero de documentos en los que aparece sin contar sus repeticiones\n",
    "    # dentro de un documento\n",
    "    contar = contarPalabras(unicas)\n",
    "    # APlicamos la formula del idf a las tuplas\n",
    "    IDF = contar.map(lambda x: (x[0],math.log10(1+(tam/x[1]))))\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cff2a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tu', 0.6020599913279624), ('nombre', 0.6020599913279624), ('claudia', 0.6020599913279624)]\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos la funcion para nuestro coprus\n",
    "# Y mostramos las 3 palabras con mayos idf\n",
    "IDF_corpus = idf(preprocesado)\n",
    "print(IDF_corpus.takeOrdered(3, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b37e8b",
   "metadata": {},
   "source": [
    "### **TF-IDF** (Inverse Document Frequency): \n",
    "    \n",
    "### $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b137150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para encontrar el tfidf de una palabra en cada documento\n",
    "aux = list(IDF_corpus.collect())\n",
    "def tf_idf(TF):\n",
    "    \"\"\"Permite hallar el TF-IDF(Inverse Document Frecuency) de las palabras de un corpus.\n",
    "    Args:\n",
    "        TF (list): Lista de listas con los TF's por documento.\n",
    "    Returns:\n",
    "        (RDD list): Una lista de listas de tuplas compuesto por (palabra, TF-IDF).\n",
    "                    Cada lista de la lista, representa un documento\n",
    "    \"\"\"\n",
    "    def find_idf(palabra):\n",
    "        \"\"\"Permite hallar el IDF(Inverse Document Frecuency) de las palabras de un corpus.\n",
    "        Args:\n",
    "            palabra (string) : Palabra de la cual queremos obtener el IDF.\n",
    "        Returns:\n",
    "            valor (float): Valor de idf de la palabra.\n",
    "        \"\"\"\n",
    "        # Para cada palabra, la buscamos en el arreglo idf, para recuperar su valor, usando la funcion filter\n",
    "        valor = list(filter(lambda x: x[0]==palabra,aux))\n",
    "        return valor[0][1]\n",
    "    # Para cada palabra de cada documento almacenado en el TF, obtenemos el TF-IDF\n",
    "    result = TF.map(lambda x: [(word[0],word[1]*float(find_idf(word[0]))) for word in x])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afbdeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_CORPUS = tf_idf(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc5a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hola', 0.2342468675289098), ('como', 0.11979187908506814), ('estas', 0.11979187908506814)]\n",
      "[('hola', 0.013774377185074694), ('buenos', 0.02754875437014939), ('dias', 0.02754875437014939), ('bien', 0.20878687094906243), ('y', 0.01820873619052574), ('tu', 0.02754875437014939), ('como', 0.01820873619052574), ('estas', 0.01820873619052574)]\n",
      "[('hola', 0.0291728207956116), ('mi', 0.0583456415912232), ('nombre', 0.0583456415912232), ('es', 0.0583456415912232), ('claudia', 0.0583456415912232), ('y', 0.03856437141683326), ('el', 0.0583456415912232), ('tuyo', 0.0583456415912232)]\n"
     ]
    }
   ],
   "source": [
    "# Mostramos los valores del TF-IDF, para las palabras de cada documento\n",
    "for i in (TF_IDF_CORPUS.collect()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e3c8d",
   "metadata": {},
   "source": [
    "### 4. N-Grams\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4837cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(n,listaTokens):\n",
    "    \"\"\"Permite hallar una lista con gramas de tamaño n.\n",
    "        Args:\n",
    "            listaTokens (array) : Lista de palabras preprocesadas, obtenidas de un texto.\n",
    "        Returns:\n",
    "            n_grams (array): Lista de gramas de tamaño n para la lista de tokens.\n",
    "    \"\"\"\n",
    "    particiones=zip(*[listaTokens[i:] for i in range(0,n)])\n",
    "    # Para todas las particiones posibles, juntamos las palabras de cada una, segun el valor de n\n",
    "    n_grams=[' '.join(ngram) for ngram in particiones]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7af372c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hola', 'hola', 'hola', 'como', 'estas'], ['hola', 'buenos', 'dias', 'bien', 'bien', 'y', 'tu', 'como', 'estas'], ['hola', 'mi', 'nombre', 'es', 'claudia', 'y', 'el', 'tuyo']]\n"
     ]
    }
   ],
   "source": [
    "# Usaremos el preprocesado del item 1 como corpus\n",
    "tokens_x_texto = preprocesado.map(lambda x: x.split())\n",
    "print(tokens_x_texto.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b312a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hola hola', 'hola hola', 'hola como', 'como estas'], ['hola buenos', 'buenos dias', 'dias bien', 'bien bien', 'bien y', 'y tu', 'tu como', 'como estas'], ['hola mi', 'mi nombre', 'nombre es', 'es claudia', 'claudia y', 'y el', 'el tuyo']]\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos la funcion de n_grams a cada documento del corpus\n",
    "# Ṕara n grams con n=2\n",
    "n_grams_docs = tokens_x_texto.map(lambda x: ngrams(2,x))\n",
    "print(n_grams_docs.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed0c7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hola hola hola', 'hola hola como', 'hola como estas'], ['hola buenos dias', 'buenos dias bien', 'dias bien bien', 'bien bien y', 'bien y tu', 'y tu como', 'tu como estas'], ['hola mi nombre', 'mi nombre es', 'nombre es claudia', 'es claudia y', 'claudia y el', 'y el tuyo']]\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos la funcion de n_grams a cada documento del corpus\n",
    "# Para n grams con n=3\n",
    "n_grams_docs = tokens_x_texto.map(lambda x: ngrams(3,x))\n",
    "print(n_grams_docs.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
