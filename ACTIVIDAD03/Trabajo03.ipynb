{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Actividad 03**\n",
    "----\n",
    "> **Alumna :** Claudia Luz Rojas Soto \n",
    "\n",
    "> **Asignatura:** Minería de Datos         \n",
    "                               \n",
    "> **Docente:** Carlos Fernando Montoya Cubas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Instalación de Librerías**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Módulos a necesitar**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***findsubsets(C,n):*** Recibe como parámetros C que es conjunto de datos de los cuales queremos generar combinaciones de sus elementos y n que es la cantidad de elementos de cada combinación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos funciones de la libreria itertools que permiten realizar las combinaciones\n",
    "from itertools import chain, combinations\n",
    "def findsubsets(C,n): \n",
    "    # Devolvemos un conjunto de combinaciones de n elementos\n",
    "    return set(itertools.combinations(C,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***print_rules(rules):*** Recibe como parámetro un arreglo de reglas y las imprime con un formato preestablecido mostrando el soporte, la confianza y el sustento(lift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rules(rules):\n",
    "    # Dentro de cada arreglo de reglas encontraremos una regla con la estructura:\n",
    "    # [ index , antecedente, consecuente, support, confidence, lift]\n",
    "    for rule in rules:\n",
    "        # Mostrando estos valores con la siguiente estructura\n",
    "        print(\" ======= REGLA DE ASOCIACIÓN =======\")\n",
    "        if(str(rule[0])==1):\n",
    "            print(str(rule[0]),'--------',rule[1])\n",
    "        else:\n",
    "            print(rule[0],'--------',rule[1])\n",
    "        print('SUPPORT    = ',rule[2])\n",
    "        print('CONFIDENCE = ',rule[3])\n",
    "        print('LIFT       = ',rule[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lectura de Datos**\n",
    "---\n",
    "***spotify.npy:*** Este conjunto de datos contiene distintas playlists compuestas por distintas canciones. Leemos este conjunto de datos volviendolo una lista de lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('spotify.npy', allow_pickle=True)\n",
    "data = data.tolist()\n",
    "# Convertimos la data a una lista de listas\n",
    "playlists = [data[i] for i in data ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tamaño del dataset:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Primer item:***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Lose Control (feat. Ciara & Fat Man Scoop)', 'Toxic', 'Crazy In Love', 'Rock Your Body', \"It Wasn't Me\", 'Yeah!', 'My Boo', 'Buttons', 'Say My Name', 'Hey Ya! - Radio Mix / Club Mix', 'Promiscuous', 'Right Where You Want Me - Radio Edit Version', 'Beautiful Soul', \"Leavin'\", 'Me & U', 'Ice Box', 'Sk8er Boi', 'Run It!', 'Check On It - feat. Bun B and Slim Thug', \"Jumpin', Jumpin'\", 'Soak Up The Sun', 'Where Is The Love?', \"Stacy's Mom\", 'Just The Girl', 'Yo (Excuse Me Miss)', 'Year 3000', 'Lip Gloss', 'Everytime We Touch - Radio Edit', 'Whatcha Say', 'Miss Independent', 'Party In The U.S.A.', 'The Great Escape', 'Replay', 'Forever', 'Your Love Is My Drug', 'Closer', 'One Less Lonely Girl', 'Paper Planes', 'Mr. Brightside', 'All The Small Things', 'Beep', 'Somebody To Love', 'Dirty Little Secret', 'Baby', 'A Thousand Miles', 'Livin on Sunday', 'See You Again', 'How Do You Sleep? - Featuring Ludacris', 'This Is Me', 'My Happy Ending', 'Check Yes Juliet', 'The Great Escape']]\n",
      "\n",
      " El tamaño de la lista es: 52\n"
     ]
    }
   ],
   "source": [
    "# Mostramos un playlist del conjunto de datos\n",
    "print(playlists[0:1])\n",
    "# Mostramos el tamaño de este playlist\n",
    "print('\\n El tamaño de la lista es:',len(playlists[0:1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declaración de parametros**\n",
    "---\n",
    "- ***veces_minimas_aparece_en_lista:***  100\n",
    "- ***soporte_min:*** veces_minimas_aparece_en_lista / cantidad de playlists\n",
    "- ***confianza_min:*** 0.5\n",
    "- ***sustento_min:*** 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soporte: 0.01\n"
     ]
    }
   ],
   "source": [
    "veces_minimas_aparece_en_lista=100\n",
    "soporte_min = veces_minimas_aparece_en_lista / len(playlists)\n",
    "confianza_min =0.5\n",
    "sustento_min =1.5\n",
    "\n",
    "print(\"Soporte:\",soporte_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementar el algoritmo Apriori**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Módulo get_frequent_itemsets**\n",
    "---\n",
    "#### Input: \n",
    "- *playlists:* Arreglo de arreglos, donde cada uno de estos arreglos representa un conjunto de canciones.\n",
    "- *min_support:* Umbral de soporte.\n",
    "\n",
    "#### Output:\n",
    "- *count_item:* Diccionario con itemsets frecuentes, y sus valores support, para cada uno de estos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_itemsets(playlists, min_support):\n",
    "    # Diccionario que guardara nuestros itemsets frecuentes y su soporte\n",
    "    count_item= dict()\n",
    "    # Base: 1-itemset\n",
    "    # Recorremos cada playlist en el conjunto de playlist\n",
    "    for playlist in playlists:\n",
    "        # Recorremos cada canción del playlist\n",
    "        for song in playlist:\n",
    "            # Si la cancion no esta en nuestro diccionario, lo agregamos y empezamos a recuperar el numero de apariciones en 1\n",
    "            if (not song in count_item):\n",
    "                count_item[song] = 1\n",
    "            # Si la canción ya está en nuestro diccionario, incrementamos el número de apariciones de este en uno\n",
    "            else:\n",
    "                count_item[song] = count_item[song] + 1\n",
    "    # Luego de obtener el numero de apariciones de cada item de nuestro diccionario\n",
    "    # Calculamos el soporte de estos, y validamos si pasa el min_support\n",
    "    # En caso no pase el umbral, sacamos el item del diccionario\n",
    "    for item in list(count_item): \n",
    "        count_item[item] = count_item[item]/len(playlists)\n",
    "        if(count_item[item] < min_support):\n",
    "            count_item.pop(item)\n",
    "    \n",
    "    # Obtención de k-itemsets\n",
    "    k = 2 # Iniciamos en 2-itemsets\n",
    "    pos = 0 \n",
    "    bandera = True # Bandera que nos indicara si ya se obtuvieron todos los itemsets frecuentes\n",
    "    # Si la bandera es false termina el bucle\n",
    "    while(bandera):\n",
    "        # Recuperamos los keys del diccionario en una lista\n",
    "        lista = list(count_item)\n",
    "        # Nueva lista : Lista auxiliar que guardará los posibles nuevos itemsets frecuentes\n",
    "        nueva_lista = []\n",
    "        # Recorremos los items que existen en la lista y los emparejamos con el resto de su forma (de manera similar a una matriz)\n",
    "        # Para esto será necerario guardar la posición de la ultimo k itemset obtenido\n",
    "        # Este pos nos permitira recorrer sectores de la lista\n",
    "        # Por ejemplo, en el caso de la forma 2-itemset frec., el recorrido de la lista empezaria cuando acaben los 1-itemsets frec. \n",
    "        # Y así sucesivamente\n",
    "        for i in range(pos,len(lista)):\n",
    "            for j in range(i+1,len(lista)):\n",
    "                # Para obtener posibles itemsets tenemos dos casos, que sea 2-itemsets o k-itemsets.\n",
    "                if(k==2):\n",
    "                    # En el caso de que estemos generando 2-itemsets, mezclamos todos los 1-itemsets \n",
    "                    nuevo_item = set()\n",
    "                    nuevo_item.add(lista[i])\n",
    "                    nuevo_item.add(lista[j]) \n",
    "                    nuevo_item = list(nuevo_item)\n",
    "                    if(len(nuevo_item)==k):\n",
    "                        # Agregamos el nuevo item ordenado en la lista auxiliar\n",
    "                        nueva_lista.append(tuple(sorted(nuevo_item)))\n",
    "                else:\n",
    "                    # En el caso de los k-itemsets\n",
    "                    # Un candidato es la mezcla de los k-1 itemsets que tengan los mismos k-2 primeros items\n",
    "                    if(lista[i][:k-2]==lista[j][:k-2]):\n",
    "                        # En caso se cumpla esto, realizamos las union de los k-1 itemsets, y se agrega el candidato (ordenado) a la lista auxiliar\n",
    "                        nuevo=list(set(lista[i]).union(set(lista[j])))\n",
    "                        if not nuevo in nueva_lista:\n",
    "                            nueva_lista.append(tuple(sorted(nuevo)))\n",
    "        \n",
    "        # Recuperamos la posición donde se encuentra el primer k-1 itemset\n",
    "        pos = len(lista)\n",
    "\n",
    "        # Si la lista auxiliar tiene candidatos a itemsets frecuentes\n",
    "        # Los añadimos al diccionario inicialmente creado\n",
    "        if(len(nueva_lista)!=0):\n",
    "            # Por cada item candidato en la lista auxiliar\n",
    "            for item_nuevo in nueva_lista:\n",
    "                # ----- FALTA VALIDACIÓN\n",
    "                # Buscamos si es parte de algun playlist\n",
    "                for playlist in playlists: \n",
    "                    # Realizamos esto con la intersección de conjuntos\n",
    "                    if(set(item_nuevo).intersection(playlist) == set(item_nuevo)):\n",
    "                        # Si el item set candidato se encuentra en algun playlist lo agregamos al diccionario con un value de 1.\n",
    "                        if (not item_nuevo in count_item.keys()):\n",
    "                            count_item[item_nuevo] = 1\n",
    "                        # Caso contrario, incrementamos el value del itemset candidato\n",
    "                        else:\n",
    "                            count_item[item_nuevo] = count_item[item_nuevo] + 1 \n",
    "                # Obtenemos el support del itemset agregado (si este ha sido agregado)\n",
    "                if (item_nuevo in count_item.keys()):\n",
    "                    count_item[item_nuevo] = count_item[item_nuevo]/len(playlists)\n",
    "                    # En caso no pase el umbral de min_support lo eliminamos del diccionario \n",
    "                    if count_item[item_nuevo] < min_support:\n",
    "                        count_item.pop(item_nuevo)\n",
    "        #Caso contrario, volvemos la bandera a false, ya que no se podran obtener más itemsets frecuentes.\n",
    "        else:\n",
    "            bandera=False\n",
    "        # Agregamos 1 al tamaño del itemset que se quiere generar\n",
    "        k = k+1\n",
    "        # Generamos...\n",
    "    # Guardamos los valores del diccionario de la forma [itemset_frecuente, support] en un archivo .csv\n",
    "    df=pd.DataFrame.from_dict(count_item,orient=\"index\")\n",
    "    df.to_csv('./playlists-'+str(len(playlists))+'-'+str(min_support)+'.csv')\n",
    "    return count_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Módulo generate_association_rules**\n",
    "---\n",
    "Input: \n",
    "- *frequent_itemsets:* Diccionario de la forma donde cada elemento es de la forma: **(itemset frecuente: support)**.\n",
    "- *confidence:* Umbral de confianza, default = 0.\n",
    "- *lift:* Umbral de lift, default = 0.\n",
    "\n",
    "Output:\n",
    "- *rules:* Arreglo de reglas que guardan el antecedente, consecuente, support, confidence y lift de cada una de estas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, confidence = 0, lift = 0):   \n",
    "    # Single_tam es la posicion donde esta el ultimo 1-itemset\n",
    "    single_tam = 0\n",
    "    # Para obtener este valor recorremos las keys del diccionario\n",
    "    items = list(frequent_itemsets)\n",
    "    values = list(frequent_itemsets.values())\n",
    "    # Existe un problema al leer el diccionario desde un .csv ya que no lee tuplas si no strings\n",
    "    # La siguiente parte de codigo crea un diccionario auxiliar que nos permitira eliminar este error\n",
    "    aux_dict = dict()\n",
    "    for i in range(len(items)):\n",
    "        val = items[i]\n",
    "        if val.startswith(\"(\"):\n",
    "            val = literal_eval(val)\n",
    "            items[i] = val\n",
    "            aux_dict[val] = values[i]\n",
    "        else:\n",
    "            single_tam = single_tam + 1\n",
    "            aux_dict[items[i]] = values[i]\n",
    "    # Recuperamos las keys del diccionario de itemsets frecuentes\n",
    "    rules = []\n",
    "    # Recorremos este arreglo de atras para adelante, es decir, desde los itemsets con k elementos hasta los itemsets con 2 elementos, ayudandonos del single_tam anterior.\n",
    "    for i in range(len(items)-1,single_tam,-1):\n",
    "        # Recuperamos el tamaño del item\n",
    "        tam_item = len(items[i])\n",
    "        # Para generar las reglas empezamos a partir de un tamaño de tam_item - 1\n",
    "        tam_antecedente = tam_item - 1\n",
    "        # Mientras el tamaño del antecedente sea mayor que uno buscamos reglas\n",
    "        while tam_antecedente>=1:\n",
    "            # Generamos combinaciones a partir de los elementos de itemset frecuente con el tamaño del antecedente, predefinido anteriormente\n",
    "            subconjuntos = list(findsubsets(items[i],tam_antecedente))\n",
    "            # Recogemos el valor del support del item analizado\n",
    "            x_u_y = aux_dict[items[i]]\n",
    "            # Para cada combinación con k (tam_antecedente) items\n",
    "            for elemento in subconjuntos:\n",
    "                # -FALTA PODA CONFIDENCE \n",
    "                # Recogemos el valor del support del antecedente      \n",
    "                if (len(elemento) == 1): \n",
    "                    x = aux_dict[str(elemento[0])] # Elemento[0] lo realizamos para recogerlo como string ya que solo es 1 elemento.\n",
    "                else:\n",
    "                    x = aux_dict[elemento]\n",
    "                # Obtenemos el consecuente\n",
    "                item_diff = tuple(sorted(list(set(items[i]).difference(elemento))))\n",
    "                # Si es solo un valor, lo recogemos como string para buscarlos\n",
    "                if(len(item_diff)==1):\n",
    "                    item_diff = list(item_diff)[0]\n",
    "                \n",
    "                # Obtenemos el valor del confidence dado por:\n",
    "                # c( x -> y ) = sup(x u y) / sup ( x )\n",
    "                confidence_ob = x_u_y/x\n",
    "                # l(x -> y) = confidence / sup (y)\n",
    "                # sup (y) = frequent_itemsets[item_diff]\n",
    "                lift_ob = confidence_ob/aux_dict[item_diff]\n",
    "                # Si los valores obtenidos pasan el umbral, agregamos la regla a nuestro arreglo de reglas con un index.\n",
    "                if (confidence_ob>= confidence and lift_ob>= lift):\n",
    "                    # Si el elemento agregado es solo uno lo volvemos string\n",
    "                    if(len(elemento)==1):\n",
    "                        rules.append([elemento[0],item_diff,x_u_y,confidence_ob,lift_ob])\n",
    "                    # Caso contrario continua como tupla\n",
    "                    else:\n",
    "                        rules.append([elemento,item_diff,x_u_y,confidence_ob,lift_ob])\n",
    "            # Reducimos el tamaño del antecedente, para generar las combinaciones con este tamaño\n",
    "            tam_antecedente = tam_antecedente - 1\n",
    "    return rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aplicar el algoritmo y obtener reglas de asociación**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La generación de los datasets encontrados en la carpeta experiments fueron realizados en paralelo en pc's. Por lo que se optó por guardar los resultados en archivos .csv para su mejor lectura. Las funciones usadas fueron las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 playlists\n",
    "get_frequent_itemsets(playlists,0.01) # 10000 playlists, support: 0.01\n",
    "get_frequent_itemsets(playlists,0.015) # 10000 playlists, support: 0.015\n",
    "get_frequent_itemsets(playlists,0.02) # 10000 playlists, support: 0.02\n",
    "get_frequent_itemsets(playlists,0.025) # 10000 playlists, support: 0.025\n",
    "# 5000 playlists\n",
    "get_frequent_itemsets(playlists[:5000],0.01) # 5000 playlists, support: 0.01\n",
    "get_frequent_itemsets(playlists[:5000],0.02) # 5000 playlists, support: 0.02\n",
    "# 2500 playlists\n",
    "get_frequent_itemsets(playlists[:2500],0.01) # 2500 playlists, support: 0.01\n",
    "get_frequent_itemsets(playlists[:2500],0.02) # 2500 playlists, support: 0.02\n",
    "# 1250 playlists\n",
    "get_frequent_itemsets(playlists[:1250],0.01) # 1250 playlists, support: 0.01\n",
    "get_frequent_itemsets(playlists[:1250],0.02) # 1250 playlists, support: 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siendo los archivos .csv , los siguientes:\n",
    " - playlists-10000-0.01.csv\n",
    " - playlists-10000-0.015.csv\n",
    " - playlists-10000-0.02.csv \n",
    " - playlists-10000-0.025.csv\n",
    " - playlists-5000-0.01.csv\n",
    " - playlists-5000-0.02.csv\n",
    " - playlists-2500-0.01.csv\n",
    " - playlists-2500-0.02.csv\n",
    " - playlists-1250-0.01.csv\n",
    " - playlists-1250-0.02.csv\n",
    "\n",
    "Identificados con la palabra 'playlists', el tamaño del dataset tomado, y el valor siguiente el min-support con el que fue evaluado este dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para producir las reglas desde nuestros itemsets frecuentes de 'playlists-10000-0.01.csv', los cuales fueron obtenidos de las 10000 primeras canciones con un support de 0.01. Ejecutamos la función generate_association_rules con un confidence de 0.5 y un lift de 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedente</th>\n",
       "      <th>Consecuente</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>No Heart</td>\n",
       "      <td>X (feat. Future)</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>33.720740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>X (feat. Future)</td>\n",
       "      <td>No Heart</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.502439</td>\n",
       "      <td>33.720740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Knee Deep (feat. Jimmy Buffett)</td>\n",
       "      <td>Chicken Fried</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.656442</td>\n",
       "      <td>30.112005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>You Was Right</td>\n",
       "      <td>Money Longer</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.587940</td>\n",
       "      <td>29.844655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Money Longer</td>\n",
       "      <td>You Was Right</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.593909</td>\n",
       "      <td>29.844655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Butterfly Effect</td>\n",
       "      <td>Bank Account</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.572165</td>\n",
       "      <td>24.769045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Bank Account</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>24.458874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(HUMBLE., Tunnel Vision)</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>22.236756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(Bounce Back, Broccoli (feat. Lil Yachty))</td>\n",
       "      <td>Bad and Boujee (feat. Lil Uzi Vert)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>21.960164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(HUMBLE., XO TOUR Llif3)</td>\n",
       "      <td>DNA.</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>21.551724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Antecedente  \\\n",
       "92                                     No Heart   \n",
       "91                             X (feat. Future)   \n",
       "105             Knee Deep (feat. Jimmy Buffett)   \n",
       "107                               You Was Right   \n",
       "106                                Money Longer   \n",
       "56                             Butterfly Effect   \n",
       "83                                     Magnolia   \n",
       "8                      (HUMBLE., Tunnel Vision)   \n",
       "31   (Bounce Back, Broccoli (feat. Lil Yachty))   \n",
       "46                     (HUMBLE., XO TOUR Llif3)   \n",
       "\n",
       "                             Consecuente  Support  Confidence       Lift  \n",
       "92                      X (feat. Future)   0.0103    0.691275  33.720740  \n",
       "91                              No Heart   0.0103    0.502439  33.720740  \n",
       "105                        Chicken Fried   0.0107    0.656442  30.112005  \n",
       "107                         Money Longer   0.0117    0.587940  29.844655  \n",
       "106                        You Was Right   0.0117    0.593909  29.844655  \n",
       "56                          Bank Account   0.0111    0.572165  24.769045  \n",
       "83                          Bank Account   0.0113    0.565000  24.458874  \n",
       "8                          XO TOUR Llif3   0.0102    0.733813  22.236756  \n",
       "31   Bad and Boujee (feat. Lil Uzi Vert)   0.0100    0.775194  21.960164  \n",
       "46                                  DNA.   0.0102    0.500000  21.551724  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets_10000_01 = pd.read_csv('https://raw.githubusercontent.com/Claudiars20/BIGDATA2021/main/ACTIVIDAD03/experiments/playlists-10000-0.01.csv', header=None, index_col=0, squeeze=True).to_dict()\n",
    "frequent_itemsets_10000_01.pop(list(frequent_itemsets_10000_01.keys())[0])\n",
    "rules_10000_01 = generate_association_rules(frequent_itemsets_10000_01,0.5,1.2)\n",
    "rules_pd_10000_01 = pd.DataFrame(rules_10000_01,columns = [\"Antecedente\", \"Consecuente\", \"Support\",\"Confidence\",\"Lift\"])\n",
    "rules_pd_10000_01= rules_pd_10000_01.sort_values('Lift',ascending=False)\n",
    "rules_pd_10000_01.to_csv(\"rules_10000_01.csv\")\n",
    "rules_pd_10000_01.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para producir las reglas desde nuestros itemsets frecuentes de 'playlists-10000-0.015.csv', los cuales fueron obtenidos de las 10000 primeras canciones con un support de 0.015. Ejecutamos la función generate_association_rules con un confidence de 0.5 y un lift de 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedente</th>\n",
       "      <th>Consecuente</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DNA.</td>\n",
       "      <td>HUMBLE.</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>17.424798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bounce Back</td>\n",
       "      <td>Bad and Boujee (feat. Lil Uzi Vert)</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.563333</td>\n",
       "      <td>15.958451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mask Off</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.515823</td>\n",
       "      <td>15.630993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mask Off</td>\n",
       "      <td>HUMBLE.</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>13.735524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>Congratulations</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.542424</td>\n",
       "      <td>13.229860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>HUMBLE.</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>13.152805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mask Off</td>\n",
       "      <td>Congratulations</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.512658</td>\n",
       "      <td>12.503859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goosebumps</td>\n",
       "      <td>Congratulations</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.503247</td>\n",
       "      <td>12.274311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>goosebumps</td>\n",
       "      <td>HUMBLE.</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.542208</td>\n",
       "      <td>11.536336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congratulations</td>\n",
       "      <td>HUMBLE.</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.521951</td>\n",
       "      <td>11.105345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Antecedente                          Consecuente  Support  Confidence  \\\n",
       "6              DNA.                              HUMBLE.   0.0190    0.818966   \n",
       "7       Bounce Back  Bad and Boujee (feat. Lil Uzi Vert)   0.0169    0.563333   \n",
       "8          Mask Off                        XO TOUR Llif3   0.0163    0.515823   \n",
       "2          Mask Off                              HUMBLE.   0.0204    0.645570   \n",
       "10    XO TOUR Llif3                      Congratulations   0.0179    0.542424   \n",
       "0     XO TOUR Llif3                              HUMBLE.   0.0204    0.618182   \n",
       "1          Mask Off                      Congratulations   0.0162    0.512658   \n",
       "5        goosebumps                      Congratulations   0.0155    0.503247   \n",
       "9        goosebumps                              HUMBLE.   0.0167    0.542208   \n",
       "3   Congratulations                              HUMBLE.   0.0214    0.521951   \n",
       "\n",
       "         Lift  \n",
       "6   17.424798  \n",
       "7   15.958451  \n",
       "8   15.630993  \n",
       "2   13.735524  \n",
       "10  13.229860  \n",
       "0   13.152805  \n",
       "1   12.503859  \n",
       "5   12.274311  \n",
       "9   11.536336  \n",
       "3   11.105345  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets_10000_015 = pd.read_csv('https://raw.githubusercontent.com/Claudiars20/BIGDATA2021/main/ACTIVIDAD03/experiments/playlists-10000-0.015.csv', header=None, index_col=0, squeeze=True).to_dict()\n",
    "frequent_itemsets_10000_015.pop(list(frequent_itemsets_10000_015.keys())[0])\n",
    "rules_10000_015 = generate_association_rules(frequent_itemsets_10000_015,0.5,1.2)\n",
    "rules_pd_10000_015 = pd.DataFrame(rules_10000_015,columns = [\"Antecedente\", \"Consecuente\", \"Support\",\"Confidence\",\"Lift\"])\n",
    "rules_pd_10000_015= rules_pd_10000_015.sort_values('Lift',ascending=False)\n",
    "rules_pd_10000_015.to_csv(\"rules_10000_01.csv\")\n",
    "rules_pd_10000_015.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para producir las reglas desde nuestros itemsets frecuentes de 'playlists-5000-0.01.csv', los cuales fueron obtenidos de las 5000 primeras canciones con un support de 0.01. Ejecutamos la función generate_association_rules con un confidence de 0.5 y un lift de 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedente</th>\n",
       "      <th>Consecuente</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Knee Deep (feat. Jimmy Buffett)</td>\n",
       "      <td>Toes</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>34.478096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Toes</td>\n",
       "      <td>Knee Deep (feat. Jimmy Buffett)</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>34.478096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>You Was Right</td>\n",
       "      <td>Money Longer</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>32.148702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Money Longer</td>\n",
       "      <td>You Was Right</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.585106</td>\n",
       "      <td>32.148702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Play It Again</td>\n",
       "      <td>That's My Kind Of Night</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>31.001984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>That's My Kind Of Night</td>\n",
       "      <td>Play It Again</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>31.001984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>(Bank Account, XO TOUR Llif3)</td>\n",
       "      <td>Magnolia</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>30.088496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Santeria</td>\n",
       "      <td>What I Got</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>30.040053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>What I Got</td>\n",
       "      <td>Santeria</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>30.040053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>(Magnolia, XO TOUR Llif3)</td>\n",
       "      <td>Bank Account</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>28.683915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Antecedente                      Consecuente  \\\n",
       "129  Knee Deep (feat. Jimmy Buffett)                             Toes   \n",
       "130                             Toes  Knee Deep (feat. Jimmy Buffett)   \n",
       "145                    You Was Right                     Money Longer   \n",
       "144                     Money Longer                    You Was Right   \n",
       "140                    Play It Again          That's My Kind Of Night   \n",
       "139          That's My Kind Of Night                    Play It Again   \n",
       "59     (Bank Account, XO TOUR Llif3)                         Magnolia   \n",
       "133                         Santeria                       What I Got   \n",
       "132                       What I Got                         Santeria   \n",
       "61         (Magnolia, XO TOUR Llif3)                     Bank Account   \n",
       "\n",
       "     Support  Confidence       Lift  \n",
       "129   0.0102    0.593023  34.478096  \n",
       "130   0.0102    0.593023  34.478096  \n",
       "145   0.0110    0.604396  32.148702  \n",
       "144   0.0110    0.585106  32.148702  \n",
       "140   0.0100    0.520833  31.001984  \n",
       "139   0.0100    0.595238  31.001984  \n",
       "59    0.0102    0.680000  30.088496  \n",
       "133   0.0108    0.504673  30.040053  \n",
       "132   0.0108    0.642857  30.040053  \n",
       "61    0.0102    0.728571  28.683915  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets_5000_01 = pd.read_csv('https://raw.githubusercontent.com/Claudiars20/BIGDATA2021/main/ACTIVIDAD03/experiments/playlists-5000-0.01.csv?token=AOMD44D5BJMWDNNBYM56ZZLBUQTV4', header=None, index_col=0, squeeze=True).to_dict()\n",
    "frequent_itemsets_5000_01.pop(list(frequent_itemsets_5000_01.keys())[0])\n",
    "rules_5000_01 = generate_association_rules(frequent_itemsets_5000_01,0.5,1.2)\n",
    "rules_pd_5000_01 = pd.DataFrame(rules_5000_01,columns = [ \"Antecedente\", \"Consecuente\", \"Support\",\"Confidence\",\"Lift\"])\n",
    "rules_pd_5000_01 = rules_pd_5000_01.sort_values('Lift',ascending=False)\n",
    "rules_pd_5000_01.to_csv(\"rules_5000_01.csv\",header=None)\n",
    "rules_pd_5000_01.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para producir las reglas desde nuestros itemsets frecuentes de 'playlists-2500-0.01.csv', los cuales fueron obtenidos de las 2500 primeras canciones con un support de 0.01. Ejecutamos la función generate_association_rules con un confidence de 0.5 y un lift de 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedente</th>\n",
       "      <th>Consecuente</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Knee Deep (feat. Jimmy Buffett)</td>\n",
       "      <td>Toes</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>38.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Toes</td>\n",
       "      <td>Knee Deep (feat. Jimmy Buffett)</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>38.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(Mask Off, iSpy (feat. Lil Yachty))</td>\n",
       "      <td>Rolex</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>35.450936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Rolex</td>\n",
       "      <td>(Mask Off, iSpy (feat. Lil Yachty))</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>35.450936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>What I Got</td>\n",
       "      <td>Santeria</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>34.473249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Santeria</td>\n",
       "      <td>What I Got</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>34.473249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>That's My Kind Of Night</td>\n",
       "      <td>Play It Again</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>33.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Play It Again</td>\n",
       "      <td>That's My Kind Of Night</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>33.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Bank Account, Butterfly Effect)</td>\n",
       "      <td>rockstar</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>33.674569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Bank Account, rockstar)</td>\n",
       "      <td>Butterfly Effect</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>33.674569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Antecedente                          Consecuente  \\\n",
       "171      Knee Deep (feat. Jimmy Buffett)                                 Toes   \n",
       "172                                 Toes      Knee Deep (feat. Jimmy Buffett)   \n",
       "29   (Mask Off, iSpy (feat. Lil Yachty))                                Rolex   \n",
       "31                                 Rolex  (Mask Off, iSpy (feat. Lil Yachty))   \n",
       "169                           What I Got                             Santeria   \n",
       "170                             Santeria                           What I Got   \n",
       "108              That's My Kind Of Night                        Play It Again   \n",
       "109                        Play It Again              That's My Kind Of Night   \n",
       "17      (Bank Account, Butterfly Effect)                             rockstar   \n",
       "18              (Bank Account, rockstar)                     Butterfly Effect   \n",
       "\n",
       "     Support  Confidence       Lift  \n",
       "171   0.0104    0.619048  38.690476  \n",
       "172   0.0104    0.650000  38.690476  \n",
       "29    0.0100    0.609756  35.450936  \n",
       "31    0.0100    0.581395  35.450936  \n",
       "169   0.0100    0.675676  34.473249  \n",
       "170   0.0100    0.510204  34.473249  \n",
       "108   0.0104    0.702703  33.783784  \n",
       "109   0.0104    0.500000  33.783784  \n",
       "17    0.0100    0.781250  33.674569  \n",
       "18    0.0100    0.781250  33.674569  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets_2500_01 = pd.read_csv('https://raw.githubusercontent.com/Claudiars20/BIGDATA2021/main/ACTIVIDAD03/experiments/playlists-2500-0.01.csv?token=AOMD44H5U2KAWPTL7G6PH4DBUQQQS', header=None, index_col=0, squeeze=True).to_dict()\n",
    "frequent_itemsets_2500_01.pop(list(frequent_itemsets_2500_01.keys())[0])\n",
    "rules_2500_01= generate_association_rules(frequent_itemsets_2500_01,0.5,1.2)\n",
    "rules_pd_2500_01 = pd.DataFrame(rules_2500_01,columns = [ \"Antecedente\", \"Consecuente\", \"Support\",\"Confidence\",\"Lift\"])\n",
    "rules_pd_2500_01= rules_pd_2500_01.sort_values('Lift',ascending=False)\n",
    "rules_pd_2500_01.to_csv(\"rules_2500_01.csv\")\n",
    "rules_pd_2500_01.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comentario:**\n",
    "- Para los tres casos presentados, los umbrales de support, confidence y lift fueron los mismos. El umbral de support se basa en que al menos 1 itemset frecuente debe estar en el 1% de los playlist, por tanto, en el caso del conjunto de 10000 playlist un itemset debe estar por lo menos en 100 playlists , en un conj. de 5000 playlist debe estar por lo menos en 50 y en un conjunto de 2500, debe estar por lo menos en 25, siendo elegido este umbral viendo que son pocos los casos en los que llega al 4% o 5%.\n",
    "- La obtención de las reglas depende bastante del tamaño de playlists usados, ya que estos afectan los valores de support, confidence y lift. \n",
    "- En el caso del umbral de lift, el límite de este fue tomado en 1.2, afectando en casi nada la generación de reglas, dado que la mayoria de estas sobrepasaban por mucho el valor dado. Es así que al sobrepasar 1 en este caso y obtiendose grandes cifras, basandonos en que cuanto más se aleja el valor de lift de 1, más evidencias de que la regla no se debe a un artefacto aleatorio, es decir, mayor la evidencia de que la regla representa un patrón real. Podemos decir que no es casualidad que los consecuentes de cada una de las reglas fueron pura casualidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicar las reglas obtenidas**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaremos algunas de las reglas obtenidas del dataset completo, con support 0.01 y 0.015, las cuales seran:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Caso 1:**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedente</th>\n",
       "      <th>Consecuente</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>No Heart</td>\n",
       "      <td>X (feat. Future)</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>33.72074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>X (feat. Future)</td>\n",
       "      <td>No Heart</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.502439</td>\n",
       "      <td>33.72074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Antecedente       Consecuente  Support  Confidence      Lift\n",
       "92          No Heart  X (feat. Future)   0.0103    0.691275  33.72074\n",
       "91  X (feat. Future)          No Heart   0.0103    0.502439  33.72074"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_pd_10000_01.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta regla de las 10000 playlists con un umbral de support de 0.01, no es coincidencia que una canción de ambas haga escuchar la otra, ya que ambas tienen un alto lift. \n",
    "\n",
    "***Descripcion de las canciones:***\n",
    "- **X(feat. Future):** \n",
    "> **Artistas:** 21 Savage, Metro Boomin\n",
    "\n",
    "> **Artista invitado:** Future\n",
    "\n",
    "> **Álbum:** Savage Mode\n",
    "\n",
    "> **Fecha de lanzamiento:** 2016\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "- **No Heart:**\n",
    "\n",
    "> **Artistas:** 21 Savage, Metro Boomin\n",
    "\n",
    "> **Álbum:** Savage Mode\n",
    "\n",
    "> **Fecha de lanzamiento:** 2016\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "Ambas canciones son pertenecientes al género de Hip-hop/rap. Coincidentemente, ambas canciones son partes del mismo álbum, el cual fue lanzado el 2016, por 21 Savage y Metro Boomin, grandes referentes de este género.\n",
    "\n",
    "Sobre los indicadores obtenidos tenemos que si se escucha X(feat. Future) hay aprox. el 50% de probabilidades que escuchemos No heart, lo que no sucede a la inversa, donde la probabilidad de que si escucho No Heart escuche X(feat. Future), por otro lado, el alto valor de lift evidencia que estas canciones estan muy presentes en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Caso 2:**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ======= REGLA DE ASOCIACIÓN =======\n",
      "Magnolia -------- Bank Account\n",
      "SUPPORT    =  0.0113\n",
      "CONFIDENCE =  0.565\n",
      "LIFT       =  24.458874458874458\n"
     ]
    }
   ],
   "source": [
    "print_rules([list(rules_pd_10000_01.iloc[6])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta regla se extrajó de las 10000 playlists, con un umbral de support de 0.01.\n",
    "\n",
    "***Descripcion de las canciones:***\n",
    "- **Magnolia:** \n",
    "> **Artistas:** Playboi Carti\n",
    "\n",
    "> **Álbum:** Playboi Carti\n",
    "\n",
    "> **Fecha de lanzamiento:** 2017\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "- **Bank Account:**\n",
    "\n",
    "> **Artistas:** 21 Savage\n",
    "\n",
    "> **Álbum:** Issa Album\n",
    "\n",
    "> **Fecha de lanzamiento:** 2017\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "Ambas canciones son pertenecientes al género de Hip-hop/rap. Lanzadas el mismo año por distintos artistas.\n",
    "\n",
    "Es probable que esta relación de causalidad se deba a que ambas canciones fueron lanzadas el mismo año, por otro lado el alto valor de lift muestra que ambas canciones se encuentran presentes en muchos playlist, y que en la mayoría de veces ambas canciones a parecen juntas, por lo que podríamos que decir que se sigue un patrón respecto a la música rap, como se mostró en el caso 1 también."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Caso 3:**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ======= REGLA DE ASOCIACIÓN =======\n",
      "('HUMBLE.', 'XO TOUR Llif3') -------- DNA.\n",
      "SUPPORT    =  0.0102\n",
      "CONFIDENCE =  0.5\n",
      "LIFT       =  21.551724137931036\n"
     ]
    }
   ],
   "source": [
    "print_rules([list(rules_pd_10000_01.iloc[9])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta regla se extrajo de los 10000 playlists con un umbral de support de 0.01.\n",
    "\n",
    "***Descripcion de las canciones:***\n",
    "- **Humble:** \n",
    "> **Artistas:** Kendrick Lamar\n",
    "\n",
    "> **Álbum:** DAMN\n",
    "\n",
    "> **Fecha de lanzamiento:** 2017\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "- **XO TOUR Llif3:**\n",
    "\n",
    "> **Artistas:**  Lil Uzi Vert\n",
    "\n",
    "> **Álbum:**  Luv Is Rage 2\n",
    "\n",
    "> **Fecha de lanzamiento:** 2017\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "- **DNA.:**\n",
    "\n",
    "> **Artistas:**  Kendrick Lamar\n",
    "\n",
    "> **Álbum:**  Damn\n",
    "\n",
    "> **Fecha de lanzamiento:** 2017\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "Esta regla de 2 a 1, demuestra el patrón en elección de temas musicales, ya que las 3 canciones pertenecen al mismo género. Por otro lado la primera canción del antecedente, tiene una relación muy estrecha con su consecuente ya que pertenecen al mismo album.\n",
    "\n",
    "La confianza esta en el límite de nuestro umbral, sin embargo puedo dejarnos ver que hay un 50% de probabilidad que ocurra esta regla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Caso 4:**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ======= REGLA DE ASOCIACIÓN =======\n",
      "Bounce Back -------- Bad and Boujee (feat. Lil Uzi Vert)\n",
      "SUPPORT    =  0.0169\n",
      "CONFIDENCE =  0.5633333333333334\n",
      "LIFT       =  15.958451369216244\n"
     ]
    }
   ],
   "source": [
    "print_rules([list(rules_pd_10000_015.iloc[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Esta regla se extrajó de las 10000 primeras playlists con un umbral de support de 0.015.\n",
    "\n",
    "***Descripcion de las canciones:***\n",
    "\n",
    "- **Bounce Back:**\n",
    "\n",
    "> **Artistas:** Big Sean\n",
    "\n",
    "> **Álbum:** I Decided\n",
    "\n",
    "> **Fecha de lanzamiento:** 2017\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "- **Bad and Boujee (feat. Lil Uzi Vert):**\n",
    "\n",
    "> **Artistas:**  Migos, Lil Uzi Vert\n",
    "\n",
    "> **Álbum:**  Culture\n",
    "\n",
    "> **Fecha de lanzamiento:** 2016\n",
    "\n",
    "> **Género:** Hip-hop/rap\n",
    "\n",
    "Las canciones pertenecientes a esta regla tienen una mélodía similar, dado el género al que pertenecen.  Además fueron lanzadas con fechas próximas lo cual influye en que las personas escuchen ambas. Por otro lado el alto valor de lift, para los parámetros tomados para generar esta regla, refleja que no es casualidad que se escuche Bad and Boujee después de Bounce Back. Así mismo, indagando un poco más sobre los artistas (Big Sean y Migos) tienen canciones que hicieron en colaboración. Lo cuál influye en la relación que tienen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **De forma resumida, a partir de las reglas generadas se ve que en el conjunto de datos completo, hay una gran presencia del género Hip-hop/rap en general, y el patrón se establece en la coincidencia en la elección de temas del mismo género. Por otro lado, si bien si fuera el caso de que el umbral de confidence subiría, podríamos probar que la presencia de estas relaciones serían algo comunes. Ya que no es casualidad la elección de canciones del mismo año o género dentro de un playlist.**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe06f96167c4fc64a78c238d993189072a4e72b444216e36203d6f96126eaf0a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
